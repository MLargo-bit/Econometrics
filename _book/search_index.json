[["index.html", "Econometrics About 0.1 Usage 0.2 Render book 0.3 Preview book", " Econometrics Mimi 2024-11-19 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 0.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 0.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 0.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["intro.html", "Chapter 1 Econometrics How to start? ", " Chapter 1 Econometrics Econometrics: Using statistical methods for estimating relationships in economics. It can be used to forecast and policy evaluation. It uses empirical data to test assumptions, relationships and theories. How to start? What is the question of interest? An economic model may be needed to test economic theories; they are equations stating the relationship you are looking for. It determines the variables that should be included. Intuition can be also a starting point: The relationship between wages and years of education: \\(wages=f(y\\_educ)\\). It is expected that years of education increase wages. The econometric model specify the function of the economic model: Wages depend on the years of education and a term u. \\[wages=\\beta_0+\\beta_1y\\_educ+u\\] However, not every factor that affects wages can be observed or measured. The term u represent such unobserved factors, and the measuring error of the included variables. It can never be eliminated entirely. It is also called error term or disturbance \\(\\beta_n\\) are the parameters. They give information about the relationship between the independent and dependent variable. After establishing the econometric model, we can start doing hypothesis about the parameters. Is the relationship positive, negative or zero? Then, data is required to estimate the parameters of the established model. There are different types of data structures: Cross-Sectional Data A Sample of variety of units taken in a point in time. Minor timing differences in collecting the data are ignored. In these kinds of data, the order of the observations doesn’t matter. Random sampling is needed to get better results. It means the observations drawed are independent. However, it is not always appropriate to make this assumption. Later we will understand that this aspect is important to hold the ceteris paribus assumption and establish a relationship between the variables, given everything else is constant. Random sampling is violated when population is not large enough, so the observations are not independent draws. It those cases, the same methodology has to be refined. In some cases, Random sampling can be checked if the descriptive statistics are balanced; for example, if we are making a comparison between women and men, the sample should be compound 50% women and 50% men, approximately. An example of cross-sectional data using dplyr package (Wickham et al. 2023) is shown below: ## # A tibble: 87 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth V… 202 136 none white yellow 41.9 male mascu… ## 5 Leia Or… 150 49 brown light brown 19 fema… femin… ## 6 Owen La… 178 120 brown, gr… light blue 52 male mascu… ## 7 Beru Wh… 165 75 brown light blue 47 fema… femin… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 9 Biggs D… 183 84 black light brown 24 male mascu… ## 10 Obi-Wan… 182 77 auburn, w… fair blue-gray 57 male mascu… ## # ℹ 77 more rows ## # ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Some variables can correspond to a different time period, but have a relationship with the dependent variable, so they must be included. That is not going to lead to special problems in the analysis of the data. Chapter 2 will focus on the analysis of these type of data. Time series data This type of data consist on a variable or plenty variables over time. Past events can influence the future. That is the expected behavior of series like stocks or GDP. Unlike in cross-sectional data, time series data order matters: the chronology of events holds important information for the analysis. Even lags can hold useful information. Observations rarely can be assumed to be independent across time. Cross-section methodologies can be used in time series; however, due to the nature of time series, like trends, modifications have been made in order to study time series. Data frequency is one of the characteristics of the data collected in time series. The most common ones are daily, weekly, monthly and annually. An example of a time series from the datasets package (datasets?) is displayed below: ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 Another characteristic of time series is seasonality. It shows effects like the weather. Pooled Cross Sections Cross-section and time series features. For example, a survey conducted in different years to different random samples. It helps analyzing the effects of policies, having measured the variables before and after it was implemented. The order of the data is not important. However, corresponding year of the information should be tracked. The analysis is similar to the cross section data. Although, it is important to account for secular differences in the variables. How a key relationship has changed over time? Below there is an example of these type of data: ## observation year price income ## 1 1 2022 177935.42 73153.77 ## 2 2 2022 71976.22 77992.54 ## 3 3 2022 103525.42 69557.17 ## 4 4 2022 88491.13 69557.17 ## 5 5 2022 88491.13 73153.77 ## 6 6 2022 71976.22 73153.77 ## observation year price income ## 495 495 2023 103525.42 77992.54 ## 496 496 2023 88491.13 73153.77 ## 497 497 2023 177935.42 69557.17 ## 498 498 2023 177935.42 75404.00 ## 499 499 2023 177935.42 69557.17 ## 500 500 2023 177935.42 77992.54 Panel or Longitudinal Data Every cross-sectional observation is a time series. The same individuals are tracked in time, unlike in pooled cross-section data. The same units are included. An example is provided using the gapminder package (Bryan 2023) ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. An advantage of these type of data is that having the same observations in time makes it possible to control by unobserved characteristics of the individuals. We are comparing the same individuals over time, so their unobserved characteristics are ‘included’ in the analysis. It can facilitate causal inference having more than one observation. The behavior of lags can be studied to look for the effects of a policy after a time has passed. Without the ceteris paribus assumption, the causal effect that we are looking for is still unknown. Holding the other variables fixed is crucial to determine the link between an independent variable and the dependent variable. To compare between two potential outcomes for the same individual is called counterfactual outcomes. We are considering the change in the outcome given a change in a single variable, everything else constant. The idea is to compare two different states of the world. Remember that experimental data has different characteristics than the available data (Wooldridge 2016). The ideal would be to compare all the possible scenarios for the same individual, and get the estimates that way. That is not possible, as from all the potential outcomes, only one is observable. To establish causal relationships between non-experimental data is then, a challenge. For practical aspects, randomization can solve some of the not observable variables that are not taken into account, as a good random selection avoid selection bias. For example, an experiment on how the amount of fertilizer affects yield productivity can be run, as long as the amount of fertilizer was randomly assigned to the land in the experiment. Otherwise, selection bias, as for example, yields near to the river got more fertilizer than dryer areas. Randomization helps the ceteris paribus assumption to hold. If randomization is not done, the relationship between the factors might be spurious. Even though many topics are interesting to run an experiment, not everything should be done, given costs and ethical issues (Wooldridge 2016). Then, nonexperimental data can be collected; that means, data that exist but wasn’t collected just to run the experiment. Data usually comes from surveys, as Census. Omitted factors that are relevant to establish a relationship with the independent variable are a known issue. As a matter of fact, not all the determinants of wages are easy to quantify, making the analysis more complex. Even though education can be considered an observed variable (you have a degree that guarantees you finished school), others are not quite obvious, like talents, gifts or innate abilities (Wooldridge 2016). Not including factors that are related can create bias in the relationship between the included variables. For example, the relation between education level and income can be negative if we are missing to add something to the model. If the omitted variable can be observed, adding it should solve the problem. However, if that is not the case and the variable can’t be easily measured, we can have an issue. To quantify the effects between two variables, problems inferring causality can arise. Wooldridge (2016) mentions the relationship between the minimum wage and unemployment. We can easily imply that these wages have impacts on other variables that also affect unemployment. Even considering these as time series, if other factors are controlled, the ceteris paribus assumption can still hold. References Bryan, Jennifer. 2023. Gapminder: Data from Gapminder. https://CRAN.R-project.org/package=gapminder. Wickham, Hadley, Romain Francois, Lionel Henry, Kirill Muller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wooldridge, Jeffrey M. 2016. “Introductory Econometrics: A Modern Approach 7th Ed.” "],["crossec.html", "Chapter 2 Cross-Sectional Data Simple Regression Model", " Chapter 2 Cross-Sectional Data Simple Regression Model It studies the relationship between two variables. the idea is to study how one changes in terms of the other. A simple linear regression model can be written as (Wooldridge 2016): \\[\\begin{equation} y=\\beta_0+\\beta_1x+u \\tag{2.1} \\end{equation}\\] \\(y\\) is called the dependent variable; \\(x\\) is called the independent variable or regressor. Be careful! The notion of independent here is not the same as in statistics between two random variables. \\(u\\) is called the error term. It represents other factors that affect \\(y\\) other than \\(x\\), meaning they will be unobserved. Taking differentials on Equation (2.1), we get: \\[\\Delta y=\\beta_1\\Delta x\\] if \\(\\Delta u=0\\), that means other factors different than \\(x\\) are constant. The above equation can be interpreted that the changes in \\(y\\) (\\(\\Delta y\\)) is the product of the slope, \\(\\beta_1\\) and the changes in \\(x\\) (\\(\\Delta x\\)). This slope determines the relationship between the variables and it is the main interest. \\(\\beta_0\\) is the intercept. Given that Equation (2.1) establishes a linear relationship between the variables, if \\(x\\) changes in one unit, the effect in \\(y\\) is going to be constant and equal to \\(\\beta_1\\). The correlation coefficient is a way to identify if two variables are linearly related. However, if \\(x\\) and \\(y\\) are uncorrelated, still one of them can be correlated with a function of the other. For example, \\(y\\) can be correlated with \\(x^3\\). When the error term \\(u\\) is correlated with \\(x\\) or any function of \\(x\\), the derived models would have interpretative problems and statistical inference won’t be possible. Instead of \\(Corr(x,u)=0\\), it is better to assume Equation (2.2): \\[\\begin{equation} E(u|x)=E(u) \\tag{2.2} \\end{equation}\\] Equation (2.2) means \\(u\\) is mean independent of \\(x\\). This also implies full independence between \\(u\\) and \\(x\\). The unobserved variables (\\(u\\)) are not related with the independent variables (\\(x\\)). The average unobserved variables (\\(u\\)) must be the same for all \\(x\\)’s. \\(x\\) must not depend on other characteristics (\\(u\\)). As long as \\(\\beta_0\\) is included in the equation, \\(E(u)=0\\) can be assumed. This statement is addressing the distribution of the unobserved factors of the sample. The intercept term \\(\\beta_0\\) can be redefined in Equation (2.1) to make this assumption true. As it can be seen, such restriction is not too binding. It implies normalizing the unobserved factors that affect \\(y\\) to have an average of zero in the population. Using the zero conditional mean assumption, \\(E(u|x)=0\\), leads to the population regression function (PRF), \\(E(y|x)\\) (Equation (2.3)): \\[\\begin{equation} E(u|y)=\\beta_0+\\beta_1x \\tag{2.3} \\end{equation}\\] The PRF is a linear function of \\(x\\), meaning that an increase in one-unit in \\(x\\) changes the expected value of \\(y\\) by \\(\\beta_1\\). For any given value of \\(x\\), the distribution of \\(y\\) is centered about \\(E(y|x)\\) (see Figure 2.1, taken from Wooldridge (2016)) knitr::include_graphics(&quot;./02-cross-sec/PRF.png&quot;) Figure 2.1: Population Regression Function References Wooldridge, Jeffrey M. 2016. “Introductory Econometrics: A Modern Approach 7th Ed.” "],["spatial.html", "Chapter 3 Spatial Econometrics Dimensions Introduction to Coordinates Plotting shapes Spatial effects Exploratory Spacial Data Analysis (ESDA) How to start? Which model is the best? Heteroskedasticity", " Chapter 3 Spatial Econometrics Chapter 1 gives initial concepts and definitions to start working econometrics. Nonetheless, the constant evolution of methodologies have brought new intriguing issues, like spacial econometrics. In many cases, it is expected that the variables’ location are related to the studied outcome. Fischer, Getis, et al. (2010) mentioned that adding spatial data to a model specification can help with mis-specification problems that can lead to inference problems of the models. The prediction feature of these types of models is not so straightforward. To the date, these kind of models helps to determine the outcome of a missing region. As expected, that presents concerning limitations. (I can be wrong in this). Spacial data: They have spacial references, like coordinates, longitude and latitude. A good point to start is asking yourself if it is expected a random process. In the case of spatial variables, characteristics can have spillover effects. Independent variables from other locations can affect their outcomes. That would mean the process is not random. Dimensions Geometries are build up with points, that are coordinates in a space from 2 to 4 dimensions: \\(X\\) and \\(Y\\). \\(Z\\) denoting height. \\(M\\) denoting some measurement associated to the point, like the time or the measurement error. -There are four possible cases: Bidimensional points refer to \\(X\\) and \\(Y\\): east and north (longitude and latitude). \\(XY\\). Tridimensionals: \\(XYZ\\). Tridimensionals: \\(XYM\\). Four dimensions: \\(XYZM\\). Jean Paelinck focuses on five characteristics of spacial econometrics study field: Spatial interdependence in spatial models. Asymmetry in spatial relationships. The relevance of other explanatory variables located in other spaces. Modeling the space. Introduction to Coordinates Polar and cartesian coordinates Figure 3.1: Polar and Cartesian Coordinates Figure 3.1 shows Polar (in green colour) and Cartesian (in orage colour) coordinates. To convert from Polar to Cartesian coordinates, Cosine and Sine functions are needed: \\[\\begin{split} x&amp;=r\\times cos(\\phi)\\\\\\\\ y&amp;=r\\times sin(\\phi) \\end{split}\\] To convert from Cartesian to Polar coordinates we use Pythagorean theorem and tangent’s inverse function: the arctangent: \\[\\begin{split} r&amp;=\\sqrt{x^2+y^2}\\\\\\\\ \\phi&amp;=tan^{-1}(y/x) \\end{split}\\] The answers is expressed in radians. The answer depends on the quadrant you are working on: Quadrant I: Use \\(tan^{-1}(y/x)\\). Quadrant II: Add \\(180^o\\) to \\(tan^{-1}(y/x)\\). Quadrant III: Add \\(180^o\\) to \\(tan^{-1}(y/x)\\). Quadrant VI: Add \\(360^o\\) to \\(tan^{-1}(y/x)\\). Spherical coordinates Are Cartesian coordinates in three dimensions: \\((x,y,z)\\). In Spherical coordinates are expressed as: \\((r,\\lambda, \\phi)\\), where \\(r\\) is the radious of the sphere, \\(\\lambda\\) is the longitude, measured in the plain \\((x,y)\\) from x axis, and \\(\\phi\\) is the latitude, the angle between the vector and the plain \\((x,y)\\). \\(-180^o \\le \\lambda \\le 180^o (0^o \\le \\lambda \\le 360^o)\\) \\(-90^o \\le \\phi \\le 90^o\\) Note Difference in distances can be due to the methodology to calculate them. Elipsoidal coordinates consider the spherical shape of the Earth, while projected coordinates consist in using two dimensions, as a flat surface. If the two objects are nearby, it is expected that the two methodologies give similar results. However, as distance increases, the error from selecting a method over the other increases as well. Data transformations are available, as seen above. Plotting shapes A shape file is required. The easiest way to plot is using the package sf (Pebesma 2018) (Pebesma and Bivand 2023): library(tidyverse) library(sf) system.file(&quot;gpkg/nc.gpkg&quot;, package=&quot;sf&quot;) |&gt; read_sf() -&gt; nc nc.32119 &lt;- st_transform(nc, &#39;EPSG:32119&#39;) nc.32119 |&gt; select(BIR74) |&gt; plot(graticule = TRUE, axes = TRUE) For interactive maps: library(mapview) |&gt; suppressPackageStartupMessages() mapviewOptions(fgb = FALSE) nc.32119 |&gt; mapview(zcol = &quot;BIR74&quot;, legend = TRUE, col.regions = sf.colors) library(stars) par(mfrow = c(2, 2)) par(mar = rep(1, 4)) tif &lt;- system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) x &lt;- read_stars(tif)[,,,1] image(x, main = &quot;(a)&quot;) image(x[,1:10,1:10], text_values = TRUE, border = &#39;grey&#39;, main = &quot;(b)&quot;) image(x, main = &quot;(c)&quot;) set.seed(131) pts &lt;- st_sample(st_as_sfc(st_bbox(x)), 3) plot(pts, add = TRUE, pch = 3, col = &#39;blue&#39;) image(x, main = &quot;(d)&quot;) plot(st_buffer(pts, 500), add = TRUE, pch = 3, border = &#39;blue&#39;, col = NA, lwd = 2) Spatial effects Spatial dependence The idea of this branch of knowledge is to study the spatial effects: Spacial dependence or Spacial autocorrelation: is the lack of independence in cross-sectional data. Things closer in space can have a stronger relationship. Distance between shapes is important. This can happen due to: Measurement problems of the observation units. Spatial aggregation; presence of externalities and spill-over effects. Spatial phenomena. The dependence is similar to the one present in time series; the main difference is the multidirectional dependence the spacial has, by nature. Spacial heterogeneity: Lack of stability of the relationships to study. Parameters and functional forms change with the position, so it is not homogeneous to the data. It can be solved using standard econometric techniques. If dependence and heterogeneity are observed in spatial data, they can only be solved using spatial econometrics. There is a functional relationship between what happens in one place and other. Exploratory Spacial Data Analysis (ESDA) It is another way to call spacial statistics methods. ESDA is the starting point to prove if spacial correlation is present. Conclusions from the ESDA, the hypothesis of spacial randomness can be rejected, to look for spacial conglomerates. It also helps with the spacial specification of the models. There are two types of measurements: Global measurements: Indicator of spacial autocorrelation or general similarity between regions. Its disadvantage is to average. Local measurements: These statistics are determined for each region. There is no generalization of the area. Makes possible to compare between regions if they are similar or different. Spacial autocorrelation Spacial autocorrelation: Happens when variables are correlated with the location. It means that observations that are closer have more similarities in between than the distant ones. That means that knowing the values of a location can help predicting the near ones. The degree of autocorrelation is another concept that can be looked for. It is the distance from where the observations become independent. Global autocorrelation can be measured using Moran’s I statistics and Geary’s C. The most popular local measurements are part of the Local Indicator of Spacial Association (LISA). It includes Moran’s \\(I_i\\) statistics and Geary’s \\(C_i\\) local statistics. Global statistics can identify conglomerates and spacial relationships just for all the system, but they can be disaggregated into local statistics to detect local spacial relationship paterns between regions. Moran’s I \\(H_0\\): No spacial autocorrelation; i.e. the values are randomly distributed. \\(I&gt;0\\): Positive spacial autocorrelation. The values to the distance are similar. \\(I&lt;0\\): Negative spacial autocorrelation. The values to the distance are different. \\(I=0\\): The values to the distance are randomly distributed. moran() moran.test() moran.mc() moran.plot() Geary’s C \\(H_0\\): No spacial autocorrelation; i.e. the values are randomly distributed. \\(0&lt;C&gt;1\\): Positive spacial autocorrelation. The values to the distance are similar. \\(1&lt;C&lt;2\\): Negative spacial autocorrelation. The values to the distance are different. geary() geary.test() geary.mc() Assumptions Estationarity. The analized data must be normal distributed with constant mean and variance. How to start? The OLS regression is a good way to start. ESDA can be performed, starting with plotting the dependent variable and see how it is distributed around the shapes. OLS We can begin assuming the model is non-spatial. \\[y=X\\beta+\\varepsilon\\] library(sf) library(tidyverse) library(tidycensus) library(corrr) library(tmap) ## Breaking News: tmap 3.x is retiring. Please test v4, e.g. with ## remotes::install_github(&#39;r-tmap/tmap&#39;) library(spdep) ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` library(tigris) ## To enable caching of data, set `options(tigris_use_cache = TRUE)` ## in your R script or .Rprofile. library(rmapshaper) library(flextable) ## ## Attaching package: &#39;flextable&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## compose library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode library(spatialreg) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;spatialreg&#39; ## The following objects are masked from &#39;package:spdep&#39;: ## ## get.ClusterOption, get.coresOption, get.mcOption, ## get.VerboseOption, get.ZeroPolicyOption, set.ClusterOption, ## set.coresOption, set.mcOption, set.VerboseOption, ## set.ZeroPolicyOption library(stargazer) ## ## Please cite as: ## Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables. ## R package version 5.2.3. https://CRAN.R-project.org/package=stargazer # Bring in census tract data. wa.tracts &lt;- get_acs(geography = &quot;tract&quot;, year = 2019, variables = c(tpop = &quot;B01003_001&quot;, tpopr = &quot;B03002_001&quot;, nhwhite = &quot;B03002_003&quot;, nhblk = &quot;B03002_004&quot;, nhasn = &quot;B03002_006&quot;, hisp = &quot;B03002_012&quot;, unemptt = &quot;B23025_003&quot;, unemp = &quot;B23025_005&quot;, povt = &quot;B17001_001&quot;, pov = &quot;B17001_002&quot;, colt = &quot;B15003_001&quot;, col1 = &quot;B15003_022&quot;, col2 = &quot;B15003_023&quot;, col3 = &quot;B15003_024&quot;, col4 = &quot;B15003_025&quot;, mobt = &quot;B07003_001&quot;, mob1 = &quot;B07003_004&quot;), state = &quot;WA&quot;, survey = &quot;acs5&quot;, output = &quot;wide&quot;, geometry = TRUE) ## Getting data from the 2015-2019 5-year ACS ## Downloading feature geometry from the Census website. To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`. ## | | | 0% | |= | 1% | |== | 3% | |=== | 4% | |==== | 6% | |===== | 8% | |====== | 8% | |======= | 9% | |======= | 11% | |======== | 11% | |========= | 13% | |========== | 14% | |========== | 15% | |============= | 19% | |============== | 20% | |=============== | 22% | |================ | 22% | |================= | 24% | |================== | 25% | |================== | 26% | |=================== | 27% | |==================== | 28% | |===================== | 30% | |====================== | 32% | |======================= | 33% | |======================== | 35% | |========================= | 36% | |========================== | 37% | |=========================== | 38% | |============================ | 40% | |============================= | 41% | |============================== | 43% | |=============================== | 44% | |================================ | 45% | |================================= | 47% | |================================= | 48% | |================================== | 49% | |=================================== | 51% | |==================================== | 52% | |====================================== | 54% | |======================================= | 55% | |======================================== | 57% | |========================================= | 58% | |=========================================== | 61% | |============================================ | 63% | |============================================== | 66% | |=============================================== | 68% | |================================================ | 69% | |================================================== | 71% | |=================================================== | 73% | |==================================================== | 74% | |===================================================== | 76% | |====================================================== | 78% | |======================================================= | 79% | |========================================================== | 82% | |=========================================================== | 85% | |============================================================ | 86% | |============================================================= | 87% | |============================================================== | 88% | |=============================================================== | 90% | |================================================================ | 91% | |================================================================== | 95% | |=================================================================== | 96% | |==================================================================== | 98% | |===================================================================== | 98% | |======================================================================| 100% # calculate percent race/ethnicity, and keep essential vars. wa.tracts &lt;- wa.tracts %&gt;% rename_with(~ sub(&quot;E$&quot;, &quot;&quot;, .x), everything()) %&gt;% #removes the E mutate(pnhwhite = 100*(nhwhite/tpopr), pnhasn = 100*(nhasn/tpopr), pnhblk = 100*(nhblk/tpopr), phisp = 100*(hisp/tpopr), unempr = 100*(unemp/unemptt), ppov = 100*(pov/povt), pcol = 100*((col1+col2+col3+col4)/colt), pmob = 100-100*(mob1/mobt)) %&gt;% dplyr::select(c(GEOID,tpop, pnhwhite, pnhasn, pnhblk, phisp, ppov, unempr, pcol, pmob)) # Bring in city boundary data pl &lt;- places(state = &quot;WA&quot;, year = 2019, cb = TRUE) ## Downloading: 31 kB Downloading: 31 kB Downloading: 64 kB Downloading: 64 kB Downloading: 65 kB Downloading: 65 kB Downloading: 97 kB Downloading: 97 kB Downloading: 97 kB Downloading: 97 kB Downloading: 120 kB Downloading: 120 kB Downloading: 130 kB Downloading: 130 kB Downloading: 130 kB Downloading: 130 kB Downloading: 150 kB Downloading: 150 kB Downloading: 150 kB Downloading: 150 kB Downloading: 180 kB Downloading: 180 kB Downloading: 190 kB Downloading: 190 kB Downloading: 190 kB Downloading: 190 kB Downloading: 190 kB Downloading: 190 kB Downloading: 210 kB Downloading: 210 kB Downloading: 210 kB Downloading: 210 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 240 kB Downloading: 270 kB Downloading: 270 kB Downloading: 270 kB Downloading: 270 kB Downloading: 300 kB Downloading: 300 kB Downloading: 440 kB Downloading: 440 kB Downloading: 440 kB Downloading: 440 kB # Keep Seattle city sea.city &lt;- filter(pl, NAME == &quot;Seattle&quot;) #Clip tracts using Seattle boundary sea.tracts &lt;- ms_clip(target = wa.tracts, clip = sea.city, remove_slivers = TRUE) #reproject to UTM NAD 83 sea.tracts &lt;-st_transform(sea.tracts, crs = &quot;+proj=utm +zone=10 +datum=NAD83 +ellps=GRS80&quot;) cdcfile &lt;- read_csv(&quot;https://raw.githubusercontent.com/crd230/data/master/PLACES_WA_2022_release.csv&quot;) ## Rows: 1444 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): GEOID, DEP_CrudePrev ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sea.tracts &lt;- sea.tracts %&gt;% mutate(GEOID = as.numeric(GEOID)) %&gt;% left_join(cdcfile, by = &quot;GEOID&quot;) sea.tracts %&gt;% select(DEP_CrudePrev, unempr, pmob, pcol, ppov, pnhblk, phisp, tpop) %&gt;% st_drop_geometry() %&gt;% summary() ## DEP_CrudePrev unempr pmob pcol ## Min. :17.40 Min. : 0.6803 Min. : 3.773 Min. :15.55 ## 1st Qu.:22.50 1st Qu.: 2.6297 1st Qu.:13.989 1st Qu.:51.77 ## Median :23.85 Median : 3.8276 Median :18.579 Median :67.76 ## Mean :23.49 Mean : 4.2518 Mean :22.139 Mean :62.55 ## 3rd Qu.:24.50 3rd Qu.: 5.1802 3rd Qu.:29.144 3rd Qu.:75.74 ## Max. :30.80 Max. :17.1955 Max. :68.782 Max. :87.85 ## ppov pnhblk phisp tpop ## Min. : 0.7709 Min. : 0.000 Min. : 0.6952 Min. : 1094 ## 1st Qu.: 5.5728 1st Qu.: 1.376 1st Qu.: 3.9049 1st Qu.: 4242 ## Median : 8.1990 Median : 3.495 Median : 5.6606 Median : 5162 ## Mean :11.5776 Mean : 7.207 Mean : 6.9320 Mean : 5481 ## 3rd Qu.:14.2095 3rd Qu.: 9.086 3rd Qu.: 7.9246 3rd Qu.: 6755 ## Max. :58.0670 Max. :37.869 Max. :38.2782 Max. :11293 cor.table &lt;- sea.tracts %&gt;% dplyr::select(-GEOID) %&gt;% st_drop_geometry() %&gt;% correlate() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; cor.table ## # A tibble: 10 × 11 ## term pnhwhite tpop pcol pnhasn pnhblk pmob phisp ppov ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pnhwhite NA -3.30e-2 0.793 -0.816 -0.781 -0.0663 -0.483 -0.578 ## 2 tpop -0.0330 NA 0.104 0.00915 0.0161 0.251 0.0500 -0.00239 ## 3 pcol 0.793 1.04e-1 NA -0.515 -0.655 0.171 -0.573 -0.399 ## 4 pnhasn -0.816 9.15e-3 -0.515 NA 0.440 0.207 0.131 0.563 ## 5 pnhblk -0.781 1.61e-2 -0.655 0.440 NA -0.146 0.201 0.416 ## 6 pmob -0.0663 2.51e-1 0.171 0.207 -0.146 NA -0.0165 0.492 ## 7 phisp -0.483 5.00e-2 -0.573 0.131 0.201 -0.0165 NA 0.132 ## 8 ppov -0.578 -2.39e-3 -0.399 0.563 0.416 0.492 0.132 NA ## 9 unempr -0.533 9.44e-4 -0.434 0.448 0.380 0.228 0.278 0.612 ## 10 DEP_Crude… 0.365 1.26e-1 0.276 -0.395 -0.333 0.442 0.0139 0.307 ## # ℹ 2 more variables: unempr &lt;dbl&gt;, DEP_CrudePrev &lt;dbl&gt; sea.tracts %&gt;% ggplot() + geom_histogram(aes(x=DEP_CrudePrev)) + xlab(&quot;Crude prevalance of depression&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. fit.ols.simple &lt;- lm(DEP_CrudePrev ~ ppov, data = sea.tracts) summary(fit.ols.simple) ## ## Call: ## lm(formula = DEP_CrudePrev ~ ppov, data = sea.tracts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4303 -0.7182 0.4342 1.2641 4.8577 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.75899 0.25694 88.579 &lt; 2e-16 *** ## ppov 0.06304 0.01700 3.708 0.000306 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.912 on 132 degrees of freedom ## Multiple R-squared: 0.09434, Adjusted R-squared: 0.08748 ## F-statistic: 13.75 on 1 and 132 DF, p-value: 0.0003063 fit.ols.multiple &lt;- lm(DEP_CrudePrev ~ unempr + pmob + pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts) summary(fit.ols.multiple) ## ## Call: ## lm(formula = DEP_CrudePrev ~ unempr + pmob + pcol + ppov + pnhblk + ## phisp + log(tpop), data = sea.tracts) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2035 -0.7811 0.0631 1.0208 4.6762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.908865 3.247366 4.899 2.90e-06 *** ## unempr -0.007734 0.071401 -0.108 0.913918 ## pmob 0.005815 0.016979 0.342 0.732577 ## pcol 0.045214 0.014485 3.121 0.002233 ** ## ppov 0.116605 0.023115 5.045 1.55e-06 *** ## pnhblk -0.082835 0.022933 -3.612 0.000437 *** ## phisp 0.087031 0.034115 2.551 0.011936 * ## log(tpop) 0.386229 0.387720 0.996 0.321085 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.56 on 126 degrees of freedom ## Multiple R-squared: 0.4246, Adjusted R-squared: 0.3927 ## F-statistic: 13.28 on 7 and 126 DF, p-value: 9.48e-13 qqPlot(fit.ols.multiple) ## [1] 66 104 plot(resid(fit.ols.multiple)) tm_shape(sea.tracts, unit = &quot;mi&quot;) + tm_polygons(col = &quot;DEP_CrudePrev&quot;, style = &quot;quantile&quot;,palette = &quot;Reds&quot;, border.alpha = 0, title = &quot;&quot;) + tm_scale_bar(breaks = c(0, 2, 4), text.size = 1, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_layout(main.title = &quot;Poor Mental Health Prevalence, Seattle 2017 &quot;, main.title.size = 0.95, frame = FALSE, legend.outside = TRUE, attr.outside = TRUE) Plot the residuals too. The idea is to look for cluster paterns, meaning the variable or the residuals are autocorrelated with their neighborhood. sea.tracts &lt;- sea.tracts %&gt;% mutate(olsresid = resid(fit.ols.multiple)) tm_shape(sea.tracts, unit = &quot;mi&quot;) + tm_polygons(col = &quot;olsresid&quot;, style = &quot;quantile&quot;,palette = &quot;Reds&quot;, border.alpha = 0, title = &quot;&quot;) + tm_scale_bar(breaks = c(0, 2, 4), text.size = 1, position = c(&quot;right&quot;, &quot;bottom&quot;)) + tm_layout(main.title = &quot;Residuals from linear regression in Seattle Tracts&quot;, main.title.size = 0.95, frame = FALSE, legend.outside = TRUE, attr.outside = TRUE) Then, start looking for problems in the data. To test for spatial effects, the row-standardized weights have to be measured and listed. This weight matrix is going to be called \\(W\\) in the equations below. Moran’s plot indicates there is evidence of association between the neighborhood values and seab&lt;-poly2nb(sea.tracts, queen=T) seaw&lt;-nb2listw(seab, style=&quot;W&quot;, zero.policy = TRUE) moran.plot(as.numeric(scale(sea.tracts$DEP_CrudePrev)), listw=seaw, xlab=&quot;Standardized Depression Prevalence&quot;, ylab=&quot;Neighbors Standardized Depression Prevalence&quot;, main=c(&quot;Moran Scatterplot for Depression Prevalence&quot;, &quot;in Seatte&quot;) ) The plot shows what it looks like an association. However, the Global Moran’s I is calculated for the dependent variable and the residuals. lm.morantest() is a function made for the residuals. It needs the object were the model is and the weights. The Lagrange Multiplyer test can be also performed to reach a conclusion, using the function lm.RStest() moran.mc(sea.tracts$DEP_CrudePrev, seaw, nsim=999) lm.morantest(fit.ols.multiple, seaw) The null hypothesis of independence (no spatial correlation) can be rejected, indicating the presence of spatial autocorrelation. OLS coefficients are biased and inefficient. Another model should be approached. # lm.LMtests(fit.ols.multiple, seaw, test=&#39;all&#39;) lm.RStests(fit.ols.multiple, seaw, test=&#39;all&#39;) The results given by the Lagrange Multipliers are: RSerr is testing how much the model improves if we run a spatial error model. The null hypothesis is that OLS performs better. Given the p-value, we can reject the null. A spatial error model should be tested. RSlag is testing how much the model improves if we run a spatial lag model. The null hypothesis is that the OLS performs better. Given the p-value, we can reject the null. A spatial lag model should be tested. adjRSerr is the robust test. It tries to filter some of the false positives. Given the p-value, the null hypothesis cannot be rejected. That means OLS is better. adjRSlag is the robust test for the spatial lag model. The p-value suggest that this model is better than OLS, as the null hypothesis is rejected. SARMA is testing how much the model improves if we run a spatial autoregresive model. The null hypothesis is that OLS performs better. We can reject the null, given the significance of the p-value. Spatially Lagged X (SLX) It is also called Spatial Durbin linear. It is the average value of the neighboring Xs. We include the values of independent variables of neighboring regions. It is a Local Espatial Model. \\[y=X\\beta+WX\\theta+\\varepsilon\\] reg2&lt;-lmSLX(DEP_CrudePrev ~ unempr + pmob + pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, seaw) summary(reg2) The coefficient of the variable and the lag can have different signs. The direct effect is the coefficient of the X variable, while the indirect effect is the coefficient of the lags. What is the total effect then? The impact() function gives the marginal effects, and what would the overall impact in the dependant variable be if all regions increase or decrease in one of their variables. impacts(reg2, listw=seaw) summary(impacts(reg2, listw=seaw), zstats=TRUE) It is important to check out the p-values, as the direct and indirect effect can be not significant, but significant as a total. Remember marginal effects interpretation is not the same as the coefficients! The sign of the total effects gives the sign of the effect in de dependent variable, but not the magnitude. Spatial Lag Model (SAR) (SLM) It is not the same Simultaneaous Autorregresive model, SAR. It includes the dependent variable of neighboring regions, y. It is a Global Espatial Model. As it affects the y variable, it has a feedback effect: what happens to the neighbors affects me, affecting the neighbors again and beyond. Every region of the network is being affected. \\[y=X\\beta+\\rho Wy+\\varepsilon\\] reg3&lt;- lagsarlm(DEP_CrudePrev ~ unempr + pmob + pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, seaw) summary(reg3) Remember Rho \\(\\rho\\) is the spatial lag parameter in the equation. It gives the degree in which the y values of the neighbors affect us, and if the effect is positive of negative. The coefficients can’t be interpreted, because there is a feedback effect, from the dependent variables affecting each other from the neighborhoods. Increasing any of our X affects our y, that affects the neighbors y, generating an infinite feedback effect. The impacts() function gives the total marginal effects. In this case, the argument R is needed. Given the nature of the model, where infitive feedback is expected, a number of simulations is required. We get different z-stats and p-values in every simulation (a seed should be used to get the same results). impacts(reg3, listw=seaw) summary(impacts(reg3, listw=seaw, R=500), zstats=TRUE) If the weights matrix is very large, the following approach is recommended: W &lt;- as(rwm_n, &quot;CsparseMatrix&quot;) trMC &lt;- trW(W, type=&quot;MC&quot;) The direct effect are how my independent variable affects my dependent variable. The indirect effect is telling how a change in my neighbor’s variable affects my dependent variable. Spatial Error Model (SEM) This global model models dependency in the residuals. It treats the dependence as nuisance. If there is a spatial error and an OLS model is selected instead of a SEM, the coefficients are not going to have the minimum variance (inefficient), affecting the inference. The spillover is the residual. The interpretation is that there is a missing variable that is spatially correlated. \\[\\begin{split} y&amp;=X\\beta+u\\\\\\\\ u&amp;=\\lambda Wu+\\varepsilon \\end{split}\\] fit.err&lt;-errorsarlm(DEP_CrudePrev ~ unempr + pmob+ pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, listw = seaw) summary(fit.err) pmob and ppov are positively related with the dependent variable, as they are the only significant variables. Lamdba is the lag error parameter. Given that is significant according to all test performed, it is an indicator for spatial autocorrelation in the error. That means there is a stochastic shock to the neighbors, and how that affects our own stochastic term. This coefficients can be directly interpreted as marginal effects. Spatial Hausman test Hausman test in panel data. Can include fixed effect that might be correlated with the included variables, but bias is expected if fixed effects are excluded. In panel data it is testing to see if leaving out the fixed effect causes bias the other coefficients. Are the estimates close if the fixed effects are included vs when they are not? It is a decision between bias and efficiency. Spatial Hausman test compares two models: OLS and SEM. The null hypothesis is that SEM is a good model to capture the spatial effects. Compares parameters. If they are too different, it is a sign that neither of them is correct. A spatial dependence can be present, and the SEM cannot be capturing those results. Hausman.test(fit.err) For our example, we can reject the null hypothesis at a 95% confidence level that the SEM is a good model for the data. We should explore a different model. Spatial Durbin Model (SDM) Global model. It is going to have spillover effects from one region to all the data set, even if they are not specified as neighbors. \\[y=\\rho Wy+X\\beta+WX\\theta+\\varepsilon\\] reg6&lt;-lagsarlm(DEP_CrudePrev ~ unempr + pmob+ pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, listw = seaw, type=&#39;mixed&#39;) summary(reg6) \\(\\rho\\) gives the impact of the neighboring y values. The marginal effects are needed in order to interpretate these models. Spatial Durbin Error Model (SDEM) It is a local model. If something changes, only impacts the direct neighbors and not all the data set. The independent variables from the neighbors are added as lags. \\[\\begin{split} y&amp;=X\\beta+WX\\theta+u\\\\\\\\ u&amp;=\\lambda Wu+\\varepsilon \\end{split}\\] reg5&lt;-errorsarlm(DEP_CrudePrev ~ unempr + pmob+ pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, listw = seaw, etype=&#39;emixed&#39;) summary(reg5) \\(\\lambda\\) is the multiplier on the residuals. If there is a higher unexplained value for the y variable in a neighboring, that increases/decreases our residual for our y variable. The total of the direct and indirect effect is different from the sum of them because of the error structure. If the direct effect is significant, there is an effect of my independent variable in my dependent variable. If the indirect effect is significant, there is an effect of the independent variable of my neighbor in my dependent variable. summary(impacts(reg5, listw=seaw), zstats=TRUE) Manski model Includes everything. \\[\\begin{split} y&amp;=\\rho Wy+X\\beta+WX\\theta+u\\\\\\\\ u&amp;=\\lambda Wu+\\varepsilon \\end{split}\\] reg7&lt;-sacsarlm(DEP_CrudePrev ~ unempr + pmob+ pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, listw = seaw, type=&#39;sacmixed&#39;) summary(reg7) SARAR model (Kelejian-Prucha) (SAC) (Cliff-Ord) Includes the error multiplier and the effects that the y variable in my neighbors have in me. It does not include the lags of the X’s. \\[\\begin{split} y&amp;=\\rho Wy+X\\beta+u\\\\\\\\ u&amp;=\\lambda Wu+\\varepsilon \\end{split}\\] reg8&lt;-sacsarlm(DEP_CrudePrev ~ unempr + pmob+ pcol + ppov + pnhblk + phisp + log(tpop), data = sea.tracts, listw = seaw, type=&#39;sac&#39;) summary(reg8) \\(\\rho\\) tells us the lag y and \\(\\lambda\\) the lag error. Which model is the best? Is it a global or local model? Start with SDM (global) or SDEM (local). There is no easy way to compare global models and local models. After answering the above, a likelihood ratio test can suggest if it is better a nested (restricted) or an unnested model (unrrestricted). Should we simplify the model? You can’t compare two unrestricted models with this test!!! Use information criteria. For Global SDM SLX SAR SEM OLS The null hypothesis is that the model should be restricted, i.e. the restricted coefficients are 0. The order of the arguments in the LR.Sarlm() function is not important to the result interpretation. LR.Sarlm(reg6, reg2) The p-value suggest the null hypothesis should be rejected. SDM model should not be restricted, comparing to the SLX. The corresponding test to compare each of the models follows. For Local SDEM SEM SLX OLS The null hypothesis is that the model should be restricted, i.e. the restricted coefficients are 0. The order of the arguments in the LR.Sarlm() function is not important to the result interpretation. LR.Sarlm(reg5, fit.err) A p-value near to 0 suggest that the null hypothesis is rejected. The non-restricted model is better. SDEM model should be used over the SEM. The degrees of freedom are how many coefficients we are restricting to 0. So it should be equal to the number of lagged independent variables. LR.Sarlm(reg5, reg2) Given the p-value, the SDEM model should not be restricted to the SLX. Other SARAR SEM SAR OLS Heteroskedasticity One last step before finishing. Do a spatial Breusch-Pagan test. The null hypothesis is the presence of heteroskedasticity. bptest.Sarlm(reg5, studentize=TRUE) In the presence of heteroskedasticity, the advise is that the p-values are going to be affected, affecting the standard error, but not the coefficients. Probably, it is not going to cause the p-values to be far enough to be concerned. Still, be careful if the p-values are too near the threshold of 0.1, 0.05 or 0.01, depending on what is the confidence interval used. References Fischer, Manfred M, Arthur Getis, et al. 2010. Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications. Springer. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009. Pebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016. "],["footnotes-and-citations.html", "Chapter 4 Footnotes and citations 4.1 Footnotes 4.2 Citations", " Chapter 4 Footnotes and citations 4.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 4.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2024) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. This is a footnote.↩︎ "],["blocks.html", "Chapter 5 Blocks 5.1 Equations 5.2 Theorems and proofs 5.3 Callout blocks", " Chapter 5 Blocks 5.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{5.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (5.1). 5.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 5.1. Theorem 5.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 5.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["sharing-your-book.html", "Chapter 6 Sharing your book 6.1 Publishing 6.2 404 pages 6.3 Metadata for sharing", " Chapter 6 Sharing your book 6.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 6.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 6.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook "],["references.html", "References", " References Bryan, Jennifer. 2023. Gapminder: Data from Gapminder. https://CRAN.R-project.org/package=gapminder. Fischer, Manfred M, Arthur Getis, et al. 2010. Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications. Springer. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009. Pebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016. Wickham, Hadley, Romain Francois, Lionel Henry, Kirill Muller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wooldridge, Jeffrey M. 2016. “Introductory Econometrics: A Modern Approach 7th Ed.” Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2024. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

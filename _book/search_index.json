[["time-series.html", "Chapter 4 Time Series 4.1 Characteristics 4.2 Static models 4.3 Finite Distributed Lag Models (FDL) 4.4 OLS properties", " Chapter 4 Time Series Most of the notes here come from Wooldridge (2016). Applying OLS method to time series requieres special attention. Often, these models violate the Gauss-Markov assumptions. We can no longer assume random sampling. 4.1 Characteristics In time series, there is temporal ordering. That means the past can affect the future, but not the other way round. The observed data is only one possible outcome of the stochastic process. These is also called realizations of the time series process. It is not possible to observe other value, since history cannot be changed. However, the existance of an stochastic process suggest that if events had been different, a different realization would have been obtained. That is why time series can be think of as the outcome of random variables. All possible realizations of an stochastic process are the total population in cross-sectional data, while the sample size is the number of periods over the observed variables. 4.2 Static models All variables are dated contemporaneously: \\[y_i = \\beta_0 + \\beta_1x_t + u_t \\text{, t = 1,2,...,n}\\] This kind of model is used if it is believed that \\(x\\) has an immediate effect on \\(y\\). 4.3 Finite Distributed Lag Models (FDL) Some variables are allowed to affect \\(y\\) with a lag. \\[\\begin{equation} y_t=\\alpha+\\beta_0x_t+\\beta_1x_{t-1}+\\beta_2x_{t-2}+u_t \\tag{4.1} \\end{equation}\\] Equation (4.1) is an FDL of order two, since the lags of the \\(x\\) variable are included from two periods before. 4.3.1 Temporal increase Imagine \\(x\\) increases in one unit at time \\(t\\), but goes back to its previos value after and the stochastic process is described as in Equation (4.1). \\(\\beta_0\\) is the immediate change in \\(y_t\\) due to a temporal increase of one unit in \\(x\\). All happening at time \\(t\\). (\\(y_t-y_{t-1}=\\beta_0\\), assuming \\(x\\) is a constant (ceteris paribus) in every period except in \\(t\\), where it is \\(x+1\\)). \\(\\beta_0\\) can be called the impact multiplier. It is the change \\(y\\) one period after the change. \\(\\beta_2\\) is the change in \\(y\\) two periods after the change, as figure 4.1 implies. However, as lags from three periods ago were not included, at time \\(t+3\\), \\(y\\) has reverted back to its initial level of all constants. Remember this was just a temporary increase. knitr::include_graphics(&quot;./03-time-ser/changes.png&quot;) Figure 4.1: Laged parameters 4.3.2 Permanent increase This time, \\(x\\) will change at period \\(t\\), without going back to its previos value. The immediate change in this situation is the same as in the temporal increase. Things change for periods greater than \\(t\\). Considering Equation (4.1), after one period, \\(y\\) is going to increase \\(\\beta_0+\\beta_1\\) and \\(\\beta+0+\\beta_1+\\beta_2\\) after two periods. Considering it is a second order FDL, there are no further changes after two periods. Then, that makes \\(\\beta+0+\\beta_1+\\beta_2\\) the long-run multiplier. For a horizon \\(h\\), the cumulative effect can be defined as the sum of the coefficients: \\(\\beta_0+\\beta_1+\\beta_2+...+\\beta_h\\). It is the expected change in the outcome after \\(h\\) periods after a permanent change in one unit increase in \\(x\\). Correlation between the \\(x\\) and its lag values is expected (implying multicollinearity) it can be challenging to get accurate estimates of each \\(\\beta\\) parameter. 4.4 OLS properties If assumptions explaned in sections 4.4.1.1, 4.4.1.2, 4.4.1.3, 4.4.2.1 and 4.4.2.2 of the Gauss-Markov theorem, we have that: The variance of \\(\\hat{\\beta_j}\\) conditional on X is \\[Var(\\hat{\\beta_j}|X = \\frac{\\sigma^2}{SST_j(1-R_j^2)}), j=1,...,k\\] 4.4.1 Unbiased estimators \\[E(\\hat{\\beta_j}) = \\beta_j\\] The expected value of the estimated parameter is the actual parameter. 4.4.1.1 Linear Parameters The time series follows a model that is linear in its parameters: \\[y_t = \\beta_0 + \\beta_1x_t+...+\\beta_kx_{tk}+u_t\\] 4.4.1.2 No perfect Collinearity No independent variable is constant nor a perfect linear combination of the others. 4.4.1.3 Zero conditional mean For each \\(t\\), the expected value of the error \\(u_t\\) given the explanatory variables for all time periods, is zero. \\[E(u_t|X) = 0,\\text{ t}=1,2,...,n\\] The above meaning the error term and the independent variables are not correlated in any time period. If the expected value of the error given the X’s in a time \\(t\\) is zero, this implies the X’s are contemporaneously exogenous. When the expected value of the error given the X’s is zero for all \\(t\\), that implies the X’s are strictly exogenous. This also means there is no restriction on correlation in the independent variables or the errors accross time. In practice, holding strict exogeneity can be unrealistic. 4.4.2 Minimum variance of estimators 4.4.2.1 Homoskedasticity The conditional variance of the error term given X is constant. \\[Var(u_t|X)=Var(u_t)\\] If this doesn’t hold, the errors are heteroskedastic. 4.4.2.2 No serial Correlation The autocorrelation of the error conditional to X is zero for all different t. \\[Corr(u_t, u_s|X)=0, \\text{ for all} t \\neq s\\] When this false, the errors are autocorrelated. The presence of autocorrelation in the errors means that if \\(y\\) is unexpectedly high for this period, then it is likely to be above average in the next period too. In many applications, this is reasonable to assume. References Wooldridge, Jeffrey M. 2016. “Introductory Econometrics: A Modern Approach 7th Ed.” "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
